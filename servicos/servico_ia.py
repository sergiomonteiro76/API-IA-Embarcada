"""
Servi√ßo de Intelig√™ncia Artificial - VERS√ÉO SIMPLIFICADA
Este m√≥dulo usa modelos pequenos e leves do Hugging Face.

VANTAGENS:
- Download r√°pido (~100MB ao inv√©s de 3GB)
- Instala√ß√£o simples
- Funciona em qualquer computador
- F√°cil de desinstalar
"""
# Adicione esta linha no in√≠cio do arquivo
import torch

# O restante do arquivo permanece o mesmo
from transformers import pipeline
import warnings

# Suprime avisos desnecess√°rios
warnings.filterwarnings('ignore')

# ==================== CACHE DOS MODELOS ====================

_cache_modelos = {}

def obter_modelo_sentimento():
    """
    Obt√©m o pipeline de an√°lise de sentimento.
    Usa um modelo MUITO LEVE e r√°pido.
    """
    if 'sentimento' not in _cache_modelos:
        print("üì• Carregando modelo de sentimento (pequeno e r√°pido)...")
        # Modelo leve de an√°lise de sentimento em portugu√™s
        # Tamanho: ~50MB
        _cache_modelos['sentimento'] = pipeline(
            "sentiment-analysis",
            model="lxyuan/distilbert-base-multilingual-cased-sentiments-student",
            device=-1  # Sempre usa CPU (mais compat√≠vel)
        )
        print("‚úÖ Modelo de sentimento carregado!")
    return _cache_modelos['sentimento']

def obter_modelo_geracao():
    """
    Obt√©m o modelo de gera√ß√£o de texto.
    Usa um modelo pequeno e eficiente.
    """
    if 'geracao' not in _cache_modelos:
        print("üì• Carregando modelo de gera√ß√£o (pequeno e r√°pido)...")
        # Modelo GPT-2 pequeno em portugu√™s
        # Tamanho: ~50MB
        _cache_modelos['geracao'] = pipeline(
            "text-generation",
            model="pierreguillou/gpt2-small-portuguese",
            device=-1  # Sempre usa CPU
        )
        print("‚úÖ Modelo de gera√ß√£o carregado!")
    return _cache_modelos['geracao']

# ==================== FUN√á√ïES DE IA ====================

def analisar_sentimento(texto):
    """
    Analisa o sentimento de um texto usando IA local.
    
    Par√¢metros:
        texto (str): O texto a ser analisado
        
    Retorna:
        dict: Dicion√°rio contendo o sentimento e a confian√ßa da an√°lise
    """
    try:
        # Obt√©m o modelo
        classificador = obter_modelo_sentimento()
        
        # Faz a an√°lise
        resultado = classificador(texto)[0]
        
        # Converte o resultado para formato padronizado
        label = resultado['label'].upper()
        confianca = resultado['score']
        
        # Mapeia os labels para portugu√™s
        mapa_sentimentos = {
            'POSITIVE': 'POSITIVO',
            'NEGATIVE': 'NEGATIVO',
            'NEUTRAL': 'NEUTRO'
        }
        
        sentimento = mapa_sentimentos.get(label, label)
        
        return {
            "sucesso": True,
            "sentimento": sentimento,
            "confianca": round(confianca * 100, 2),
            "texto_original": texto,
            "detalhes": f"{sentimento} ({confianca:.2%})"
        }
        
    except Exception as erro:
        return {
            "sucesso": False,
            "erro": str(erro),
            "mensagem": "Erro ao analisar sentimento"
        }


def gerar_texto(tema, tamanho="medio"):
    """
    Gera um texto criativo sobre um tema espec√≠fico usando IA local.
    
    Par√¢metros:
        tema (str): O tema sobre o qual gerar o texto
        tamanho (str): Tamanho do texto (curto, medio, longo)
        
    Retorna:
        dict: Dicion√°rio contendo o texto gerado
    """
    try:
        # Obt√©m o modelo
        gerador = obter_modelo_geracao()
        
        # Define o n√∫mero de tokens baseado no tamanho
        tokens_por_tamanho = {
            "curto": 50,
            "medio": 100,
            "longo": 150
        }
        
        max_tokens = tokens_por_tamanho.get(tamanho, 100)
        
        # Cria o prompt inicial
        prompt = f"{tema}."
        
        # Gera o texto
        resultado = gerador(
            prompt,
            max_new_tokens=max_tokens,
            num_return_sequences=1,
            temperature=0.7,
            top_k=50,
            top_p=0.9,
            do_sample=True,
            repetition_penalty=1.2,
            pad_token_id=50256
        )
        
        # Extrai o texto gerado
        texto_gerado = resultado[0]['generated_text']
        
        # Remove o prompt inicial se estiver presente
        if texto_gerado.startswith(prompt):
            texto_gerado = texto_gerado[len(prompt):].strip()
        
        return {
            "sucesso": True,
            "tema": tema,
            "texto_gerado": texto_gerado,
            "tamanho_solicitado": tamanho,
            "palavras_geradas": len(texto_gerado.split())
        }
        
    except Exception as erro:
        return {
            "sucesso": False,
            "erro": str(erro),
            "mensagem": "Erro ao gerar texto"
        }


def resumir_texto(texto, tamanho_resumo="medio"):
    """
    Resume um texto de forma simples (extra√ß√£o de frases principais).
    Vers√£o simplificada que n√£o requer modelos pesados.
    
    Par√¢metros:
        texto (str): O texto a ser resumido
        tamanho_resumo (str): Tamanho do resumo (curto, medio, longo)
        
    Retorna:
        dict: Dicion√°rio contendo o resumo do texto
    """
    try:
        # Resumo simples: pega as primeiras frases
        frases = texto.split('.')
        frases = [f.strip() for f in frases if f.strip()]
        
        # Define quantas frases incluir baseado no tamanho
        num_frases = {
            "curto": 1,
            "medio": 2,
            "longo": 3
        }
        
        n = num_frases.get(tamanho_resumo, 2)
        resumo = '. '.join(frases[:n]) + '.'
        
        return {
            "sucesso": True,
            "texto_original": texto,
            "resumo": resumo,
            "tamanho_resumo": tamanho_resumo,
            "reducao": f"{len(resumo)}/{len(texto)} caracteres",
            "nota": "Resumo por extra√ß√£o de frases (m√©todo simplificado)"
        }
        
    except Exception as erro:
        return {
            "sucesso": False,
            "erro": str(erro),
            "mensagem": "Erro ao resumir texto"
        }


def obter_informacoes_modelo():
    """
    Retorna informa√ß√µes sobre os modelos de IA sendo utilizados.
    
    Retorna:
        dict: Informa√ß√µes sobre os modelos
    """
    return {
        "modelos": {
            "sentimento": "lxyuan/distilbert-base-multilingual-cased-sentiments-student",
            "geracao": "pierreguillou/gpt2-small-portuguese",
            "resumo": "M√©todo simplificado (extra√ß√£o de frases)"
        },
        "provedor": "Hugging Face (Modelos Leves)",
        "descricao": "Modelos pequenos e r√°pidos, ideais para aprendizado",
        "dispositivo": "CPU",
        "tamanho_total": "~100MB (muito menor que vers√µes completas)",
        "capacidades": [
            "An√°lise de sentimentos (r√°pida)",
            "Gera√ß√£o de texto em portugu√™s",
            "Resumo simples de texto",
            "Execu√ß√£o 100% local"
        ],
        "vantagens": [
            "Download r√°pido (~2 minutos)",
            "Funciona em qualquer computador",
            "Sem custos de API",
            "F√°cil de desinstalar",
            "Privacidade total"
        ]
    }


def pre_carregar_modelos():
    """
    Pr√©-carrega os modelos na inicializa√ß√£o da aplica√ß√£o.
    """
    print("\n" + "="*60)
    print("ü§ñ Pr√©-carregando modelos de IA (vers√£o leve)...")
    print("="*60)
    
    try:
        print("\n1/2 Carregando modelo de sentimento...")
        obter_modelo_sentimento()
        
        print("\n2/2 Carregando modelo de gera√ß√£o...")
        obter_modelo_geracao()
        
        print("\n" + "="*60)
        print("‚úÖ Todos os modelos foram carregados!")
        print("üí° Tamanho total: ~100MB (muito leve!)")
        print("="*60 + "\n")
        
    except Exception as erro:
        print(f"\n‚ö†Ô∏è  Aviso: Erro ao pr√©-carregar modelos: {erro}")
        print("Os modelos ser√£o carregados sob demanda.\n")

